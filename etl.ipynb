{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# ETL Processes\n",
    "Use this notebook to develop the ETL process for each of your tables before completing the `etl.py` file to load the whole datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import psycopg2\n",
    "\n",
    "import pandas as pd\n",
    "from sql_queries import *"
   ]
  },
  {
   "source": [
    "conn = psycopg2.connect(\"host=127.0.0.1 user=postgres password=nubank\")\n",
    "conn.set_session(autocommit=True)\n",
    "cur = conn.cursor()"
   ],
   "cell_type": "code",
   "metadata": {
    "editable": true
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ObjectInUse",
     "evalue": "database \"nubank\" is being accessed by other users\nDETAIL:  There are 3 other sessions using the database.\n",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mObjectInUse\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-da260512008f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcur\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"DROP DATABASE IF EXISTS nubank\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mcur\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"CREATE DATABASE nubank WITH ENCODING 'utf8' TEMPLATE template0\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# close connection to default database\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mObjectInUse\u001b[0m: database \"nubank\" is being accessed by other users\nDETAIL:  There are 3 other sessions using the database.\n"
     ]
    }
   ],
   "source": [
    "cur.execute(\"DROP DATABASE IF EXISTS nubank\")\n",
    "cur.execute(\"CREATE DATABASE nubank WITH ENCODING 'utf8' TEMPLATE template0\")\n",
    "\n",
    "# close connection to default database\n",
    "conn.close()    \n",
    "    \n",
    "\n",
    "conn = psycopg2.connect(\"host=127.0.0.1 dbname=nubank user=postgres password=nubank\")\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "source": [
    "conn = psycopg2.connect(\"host=127.0.0.1 dbname=nubank user=postgres password=nubank\")\n",
    "cur = conn.cursor()"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 43,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "for query in drop_table_queries:\n",
    "        cur.execute(query)\n",
    "        conn.commit()\n",
    "\n",
    "for query in create_table_queries:\n",
    "        cur.execute(query)\n",
    "        conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('fact_movements_table',), ('costumers_table',), ('d_week_table',), ('accounts_table',), ('city_table',), ('customers_table',), ('country_table',), ('d_month_table',), ('d_time_table',), ('d_weekday_table',), ('d_year_table',), ('pix_movements_table',), ('state_table',), ('transfer_ins_table',), ('transfer_outs_table',)]\n"
     ]
    }
   ],
   "source": [
    "#VERIFY THE DB TABLES\n",
    "cur.execute(\"select relname from pg_class where relkind='r' and relname !~ '^(pg_|sql_)';\")\n",
    "print(cur.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_function(data, query):\n",
    "        for i, row in data.iterrows():\n",
    "                cur.execute(query, list(row))\n",
    "        conn.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Process table data\n",
    "In this first part, you'll perform ETL on the tables"
   ]
  },
  {
   "source": [
    "### Insert data in accounts_table"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "accounts_df = pd.read_csv(\"./tables/tables/accounts_table.csv\")\n",
    "insert_function(accounts_df, accounts_table_insert)"
   ]
  },
  {
   "source": [
    "### Insert data in city_table"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "city_df = pd.read_csv(\"./tables/tables/city_table.csv\")\n",
    "city_df = city_df[['city_id','city','state_id']]\n",
    "insert_function(city_df, city_table_insert)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 47,
   "outputs": []
  },
  {
   "source": [
    "### Insert data in customers_table"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "customers_df = pd.read_csv(\"./tables/tables/customers_table.csv\")\n",
    "customers_df.head()\n",
    "insert_function(customers_df, customers_table_insert)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 48,
   "outputs": []
  },
  {
   "source": [
    "### Insert data in country_table"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_df = pd.read_csv(\"./tables/tables/country_table.csv\")\n",
    "country_df = country_df[[\"country_id\", \"country\"]]\n",
    "insert_function(country_df, country_table_insert)"
   ]
  },
  {
   "source": [
    "### Insert data in d_month_table"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_month_table_df = pd.read_csv(\"./tables/tables/d_month_table.csv\")\n",
    "insert_function(d_month_table_df, d_month_table_insert)"
   ]
  },
  {
   "source": [
    "### Insert data in d_time_table"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_time_table_df = pd.read_csv(\"./tables/tables/d_time_table.csv\")\n",
    "tmp_df = \"./tables/tables/tmp.csv\"\n",
    "d_time_table_df.to_csv(tmp_df,index=False , header=False)\n",
    "f = open(tmp_df, 'r')\n",
    "cur.copy_from(f, \"d_time_table\", sep=\",\")\n",
    "conn.commit()"
   ]
  },
  {
   "source": [
    "### Insert data in d_week_table"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_week_table_df = pd.read_csv(\"./tables/tables/d_week_table.csv\")\n",
    "insert_function(d_week_table_df, d_week_table_insert)"
   ]
  },
  {
   "source": [
    "### Insert data in d_weekday_table"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_weekday_table_df = pd.read_csv(\"./tables/tables/d_weekday_table.csv\")\n",
    "insert_function(d_weekday_table_df, d_weekday_table_insert)"
   ]
  },
  {
   "source": [
    "### Insert data in d_year_table"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_year_table_df = pd.read_csv(\"./tables/tables/d_year_table.csv\")\n",
    "insert_function(d_year_table_df, d_year_table_insert)"
   ]
  },
  {
   "source": [
    "### Insert data in state_table"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_table_df = pd.read_csv(\"./tables/tables/state_table.csv\")\n",
    "state_table_df = state_table_df[[\"state_id\", \"state\", \"country_id\"]]\n",
    "insert_function(state_table_df, state_table_insert)"
   ]
  },
  {
   "source": [
    "### Insert data in pix_movements_table"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "pix_movements_table_df = pd.read_csv(\"./tables/tables/pix_movements_table.csv\")\n",
    "tmp_df = \"./tables/tables/tmp.csv\"\n",
    "pix_movements_table_df.to_csv(tmp_df,index=False , header=False)\n",
    "f = open(tmp_df, 'r')\n",
    "cur.copy_from(f, \"pix_movements_table\", sep=\",\")\n",
    "conn.commit()"
   ]
  },
  {
   "source": [
    "### Insert data in transfer_ins_table"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_ins_table_df = pd.read_csv(\"./tables/tables/transfer_ins_table.csv\")\n",
    "tmp_df = \"./tables/tables/tmp.csv\"\n",
    "transfer_ins_table_df.to_csv(tmp_df,index=False , header=False)\n",
    "f = open(tmp_df, 'r')\n",
    "cur.copy_from(f, \"transfer_ins_table\", sep=\",\")\n",
    "conn.commit()"
   ]
  },
  {
   "source": [
    "### Insert data in transfer_outs_table"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_outs_table_df = pd.read_csv(\"./tables/tables/transfer_outs_table.csv\")\n",
    "tmp_df = \"./tables/tables/tmp.csv\"\n",
    "transfer_outs_table_df.to_csv(tmp_df,index=False , header=False)\n",
    "f = open(tmp_df, 'r')\n",
    "cur.copy_from(f, \"transfer_outs_table\", sep=\",\")\n",
    "conn.commit()"
   ]
  },
  {
   "source": [
    "### Insert data in fact_movements_table"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from_pix_df = pix_movements_table_df.join(accounts_df.set_index('account_id'), on='account_id',rsuffix=\"p_\")\n",
    "from_in_df = transfer_ins_table_df.join(accounts_df.set_index('account_id'), on='account_id',rsuffix=\"in_\")\n",
    "from_out_df = transfer_outs_table_df.join(accounts_df.set_index('account_id'), on='account_id',rsuffix=\"out_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_pix_df[\"in_or_out\"].replace({\"pix_out\": \"out\", \"pix_in\": \"in\"}, inplace=True)\n",
    "from_pix_df.insert(3,'type', 'pix')\n",
    "from_pix_df = from_pix_df[['id', 'account_id','customer_id','in_or_out','type' , 'pix_amount','pix_requested_at','pix_completed_at','status' ]]\n",
    "from_pix_df = from_pix_df.rename(columns={'pix_amount': 'amount', 'pix_requested_at': 'transaction_requested_at', 'pix_completed_at': 'transaction_completed_at'})\n",
    "from_in_df.insert(3,'in_or_out', 'in')\n",
    "from_out_df.insert(3,'in_or_out', 'out')\n",
    "from_in_df.insert(3,'type', 'transfer_in')\n",
    "from_out_df.insert(3,'type', 'transfer_out')\n",
    "\n",
    "from_in_df = from_in_df[['id', 'account_id','customer_id','in_or_out','type' , 'amount','transaction_requested_at','transaction_completed_at','status' ]]\n",
    "from_out_df = from_out_df[['id', 'account_id','customer_id','in_or_out','type' , 'amount','transaction_requested_at','transaction_completed_at','status' ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_movements_table = from_pix_df.append([from_in_df, from_out_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df = \"./tables/tables/tmp.csv\"\n",
    "fact_movements_table.to_csv(tmp_df,index=False , header=False)\n",
    "f = open(tmp_df, 'r')\n",
    "cur.copy_from(f, \"fact_movements_table\", sep=\",\")\n",
    "conn.commit()"
   ]
  },
  {
   "source": [
    "### Insert data in the dim_time_table\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_time_df = d_time_table_df.join(d_year_table_df.set_index('year_id'), on='year_id',rsuffix=\"y_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_time_df.head()"
   ]
  },
  {
   "source": [
    "### Close connection"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  },
  {
   "source": [
    "# Answering the questions: "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1. Create a SQL query to help Jane retrieving the monthly balance of all accounts (this query should be made using the warehouse structure before the changes you propose on 2.)\n",
    "\n",
    "Your colleague Jane Hopper, the analyst in charge of analysing customer behaviour, who directly\n",
    "consumes data from the Data Warehouse Environment, needs to get all the account's monthly\n",
    "balances between Jan/2020 and Dec/2020. She wasn't able to do it alone, and asked for your help.\n",
    "Add to your resolution the SQL query used to retrieve the data needed (the necessary tables in csv\n",
    "format were sent along with this pdf, on folder tables/). Feel free to use the dialect of your choice,\n",
    "but please specify the SQL engine.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Resolution:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Developing the query to Account Monthly Balance\n",
    "\n",
    "The original table schema has three tables that contain movements in the costumers accounts, so in order to help Jane I need to put it all together and create the expected columns in the figure bellow.\n",
    "\n",
    "![](./images/balance_table.jpg)\n",
    "\n",
    "Note: In the image above the column costumer is present, but Jane want to know the account balance so to do this the account_id column will be used insteade customer column.\n",
    "\n",
    "\n",
    "1. Using the `UNION ALL` combined with `SELECT` the result put all the data contained in the tables together appending it.\n",
    "2. We need to create the columns with INs and OUTs, in the pix transactions we have to distinct between the IN and OUT movements, where I used `CASE` to make this distinction using the column `in_or_out` present in the pix table:\n",
    "\n",
    "\n",
    "```sql \n",
    "        CASE in_or_out WHEN 'pix_out' then pix_amount ELSE 0 END as SAIDA,\n",
    "        CASE in_or_out WHEN  'pix_in' then pix_amount ELSE 0 END as ENTRADA\n",
    "``` \n",
    "In the tables of usual transactions(non pix) the distinction was not necessary, because each table contains one specific kind of transaction, so I returned the amount in the column that represents the kind of transaction and 0(zero) in the other column, like above:\n",
    "\n",
    "In table:\n",
    "```sql\n",
    "        SELECT account_id,\n",
    "        0 AS SAIDA, \n",
    "        amount as ENTRADA,\n",
    "```\n",
    "\n",
    "Out table:\n",
    "```sql\n",
    "        SELECT account_id,\n",
    "        amount as SAIDA,\n",
    "        0 AS ENTRADA,\n",
    "```\n",
    "\n",
    "3. To get the mounth column, first I converted the data to timestamp that contains the date when the transaction was completed and after with `DATE_PART` specifing month as my part of interest, I got the month:\n",
    "\n",
    "```sql\n",
    "        DATE_PART('month', to_timestamp(CAST(transaction_completed_at as numeric)/1000)) MES\n",
    "```\n",
    "\n",
    "4. The `WHERE` clause is filtering the transactions that were completed and occured between JAN/2020 and DEC/2020:\n",
    "\n",
    "```sql\n",
    "        WHERE (status = 'completed') AND (DATE_PART('year', to_timestamp(CAST(pix_completed_at as numeric)/1000))  = 2020)\n",
    "```\n",
    "\n",
    "5. With the data returned in the subquery `transactions` is necessary to sum all the In and all the Out movements separately grouping then by account_id and month, after the monthly balance is a result of the Ins minus Outs for each month and account. All this columns were rounded with two decimal places that is enough when we talk about monetary values.\n",
    "\n",
    "\n",
    "Select statement:\n",
    "```sql\n",
    "        SELECT CAST(mes AS INT), account_id,\n",
    "        ROUND(CAST(sum(entrada) AS NUMERIC),2) AS entradas_total,\n",
    "        ROUND(CAST(SUM(saida) AS NUMERIC),2) as saidas_total,\n",
    "        ROUND(CAST((sum(entrada) - SUM(saida))AS NUMERIC),2) as balanco\n",
    "\n",
    "```\n",
    "\n",
    "Group by and Order by statement:\n",
    "\n",
    "```sql\n",
    "        GROUP BY mes, account_id\n",
    "        ORDER BY account_id, mes\n",
    "\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Importing the `create_engine` from `sqlalchemy` to create the connection to our Postgres database."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine('postgresql://postgres:nubank@127.0.0.1/nubank')"
   ]
  },
  {
   "source": [
    "### Using the `pd.read_sql` and the previous created `engine` to execute our SQL query and create a pandas dataframe with the result."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Time to query execution:  1.1054751873016357\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "balance = pd.read_sql('''\n",
    "SELECT CAST(mes AS INT), \n",
    "account_id,\n",
    "ROUND(CAST(sum(entrada) AS NUMERIC),2) AS entradas_total,\n",
    "ROUND(CAST(SUM(saida) AS NUMERIC),2) as saidas_total,\n",
    "ROUND(CAST((sum(entrada) - SUM(saida))AS NUMERIC),2) as balanco\n",
    "FROM (  SELECT account_id, \n",
    "        CASE in_or_out WHEN 'pix_out' then pix_amount ELSE 0 END as SAIDA, \n",
    "        CASE in_or_out WHEN  'pix_in' then pix_amount ELSE 0 END as ENTRADA,\n",
    "        DATE_PART('month', to_timestamp(CAST(pix_completed_at as numeric)/1000)) AS MES\n",
    "        FROM pix_movements_table\n",
    "        WHERE (status = 'completed') AND (DATE_PART('year', to_timestamp(CAST(pix_completed_at as numeric)/1000))  = 2020)\n",
    "\n",
    "        UNION ALL\n",
    "\n",
    "        SELECT account_id,\n",
    "        0 AS SAIDA, \n",
    "        amount as ENTRADA,\n",
    "        DATE_PART('month', to_timestamp(CAST(transaction_completed_at as numeric)/1000)) AS MES\n",
    "        FROM transfer_ins_table\n",
    "        WHERE (status = 'completed') AND (DATE_PART('year', to_timestamp(CAST(transaction_completed_at as numeric)/1000)) = 2020)\n",
    "\n",
    "        UNION ALL\n",
    "\n",
    "        SELECT account_id,\n",
    "        amount as SAIDA,\n",
    "        0 AS ENTRADA,\n",
    "        DATE_PART('month', to_timestamp(CAST(transaction_completed_at as numeric)/1000)) AS MES\n",
    "        FROM transfer_outs_table\n",
    "        WHERE (status = 'completed') AND (DATE_PART('year', to_timestamp(CAST(transaction_completed_at as numeric)/1000)) = 2020)) as transactions\n",
    "GROUP BY mes, account_id\n",
    "ORDER BY mes, account_id\n",
    "\n",
    "''', engine)\n",
    "end = time.time()\n",
    "print(\"Time to query execution: \",end - start)"
   ]
  },
  {
   "source": [
    "### Looking into the `balance` dataframe to see the result of our query"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "    mes         account_id  entradas_total  saidas_total  balanco\n",
       "0     1   2569200459575096         6622.15       1826.30  4795.85\n",
       "1     1   2572645138169593         3314.38       1467.31  1847.07\n",
       "2     1   2969674447809961         9013.62       3569.48  5444.14\n",
       "3     1   5756422114496119         2926.91       2170.55   756.36\n",
       "4     1   5763135580788529         6460.05      11030.11 -4570.06\n",
       "5     1   6731171884115662         9660.82       5316.85  4343.97\n",
       "6     1   6754575908057409         5780.74          0.00  5780.74\n",
       "7     1   6759884497455352         4308.37       1542.19  2766.18\n",
       "8     1   7106839639082916         3062.88       1402.65  1660.23\n",
       "9     1   7399497589386025         1319.22          0.00  1319.22\n",
       "10    1   8390860978077109         7559.18      17064.75 -9505.57\n",
       "11    1   9682140161583058         6125.78      11087.89 -4962.11\n",
       "12    1  11972044772734216         4480.96        759.50  3721.46\n",
       "13    1  14823956517654278        11269.08       4940.66  6328.42\n",
       "14    1  14960923507500004         2365.23       4424.75 -2059.52\n",
       "15    1  16488801170250194         1382.95          0.00  1382.95\n",
       "16    1  17010643349938116         1446.35       1812.93  -366.58\n",
       "17    1  17352812883658752         8757.83      13354.52 -4596.69\n",
       "18    1  18661351842096488         1148.46       5642.76 -4494.30\n",
       "19    1  21312345113145264        10272.73       2697.03  7575.70"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mes</th>\n      <th>account_id</th>\n      <th>entradas_total</th>\n      <th>saidas_total</th>\n      <th>balanco</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>2569200459575096</td>\n      <td>6622.15</td>\n      <td>1826.30</td>\n      <td>4795.85</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>2572645138169593</td>\n      <td>3314.38</td>\n      <td>1467.31</td>\n      <td>1847.07</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>2969674447809961</td>\n      <td>9013.62</td>\n      <td>3569.48</td>\n      <td>5444.14</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>5756422114496119</td>\n      <td>2926.91</td>\n      <td>2170.55</td>\n      <td>756.36</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>5763135580788529</td>\n      <td>6460.05</td>\n      <td>11030.11</td>\n      <td>-4570.06</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1</td>\n      <td>6731171884115662</td>\n      <td>9660.82</td>\n      <td>5316.85</td>\n      <td>4343.97</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>1</td>\n      <td>6754575908057409</td>\n      <td>5780.74</td>\n      <td>0.00</td>\n      <td>5780.74</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>1</td>\n      <td>6759884497455352</td>\n      <td>4308.37</td>\n      <td>1542.19</td>\n      <td>2766.18</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>1</td>\n      <td>7106839639082916</td>\n      <td>3062.88</td>\n      <td>1402.65</td>\n      <td>1660.23</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>1</td>\n      <td>7399497589386025</td>\n      <td>1319.22</td>\n      <td>0.00</td>\n      <td>1319.22</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>1</td>\n      <td>8390860978077109</td>\n      <td>7559.18</td>\n      <td>17064.75</td>\n      <td>-9505.57</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>1</td>\n      <td>9682140161583058</td>\n      <td>6125.78</td>\n      <td>11087.89</td>\n      <td>-4962.11</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>1</td>\n      <td>11972044772734216</td>\n      <td>4480.96</td>\n      <td>759.50</td>\n      <td>3721.46</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>1</td>\n      <td>14823956517654278</td>\n      <td>11269.08</td>\n      <td>4940.66</td>\n      <td>6328.42</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>1</td>\n      <td>14960923507500004</td>\n      <td>2365.23</td>\n      <td>4424.75</td>\n      <td>-2059.52</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>1</td>\n      <td>16488801170250194</td>\n      <td>1382.95</td>\n      <td>0.00</td>\n      <td>1382.95</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>1</td>\n      <td>17010643349938116</td>\n      <td>1446.35</td>\n      <td>1812.93</td>\n      <td>-366.58</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>1</td>\n      <td>17352812883658752</td>\n      <td>8757.83</td>\n      <td>13354.52</td>\n      <td>-4596.69</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>1</td>\n      <td>18661351842096488</td>\n      <td>1148.46</td>\n      <td>5642.76</td>\n      <td>-4494.30</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>1</td>\n      <td>21312345113145264</td>\n      <td>10272.73</td>\n      <td>2697.03</td>\n      <td>7575.70</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "balance.head(20)"
   ]
  },
  {
   "source": [
    "## 2. Improve the data warehouse architecture and justify your changes\n",
    "\n",
    "Imagine now that you could remodel the data warehouse environment freely, keeping in mind that\n",
    "Nubank is always evolving with new products (Whatsapp Payments, PIX, phone recharge, etc) and it\n",
    "is also expanding to new countries, so our data warehouse needs to accommodate all these\n",
    "incoming changes. Which modifications would you propose and why? Remember to consider that\n",
    "other analysts will be using the same structure, so it should be as clear as possible. Feel free to\n",
    "change, remove or add tables/fields to generate a better database design"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Resolution:\n",
    "\n",
    "To do this remodeling in the data warehouse is necessary first to answer the question: **What are the fundamental goals of the data warehousing ?** So let`s talk about this.\n",
    "\n",
    "**A few bussiness analysts big problems:**\n",
    "\n",
    "- We have data, but we can`t access it.\n",
    "- We need access the data easily,\n",
    "- We need the right numbers to make the right decisions.\n",
    "\n",
    "**Based on this problems the main goals to build a data warehouse could be:**\n",
    "\n",
    "- Make the data easily accessible.\n",
    "- Show consistently information.\n",
    "- Need to be adaptable to changes.\n",
    "- Fast data aquisition.\n",
    "- Trustworthy data.\n",
    "- Data security.\n",
    "\n",
    "After this small introduction is expected that the more important aspects of a data warehouse are cleared. If not we can make it simple: We need to provide the data contained in the tables as clearly, easily and with the fastest performance as possible. To do this I will use the **Dimensional modeling** that is a very used technique for presenting analytic data.\n",
    "\n",
    "### Dimensional Modeling\n",
    "\n",
    "Dimensional modeling is used to make tha data arrangement simple, we need it simple to the users can easily understand the data, it needs to be friendly user, the dimensional models are diferent from the realtional or 3NF(third normal form) where the focus is in remove data redundances.\n",
    "\n",
    "To make this remodeling I will use A **Star Schema** model, where we have a **fact table** connected to **dimensional tables**. This is the name because it can remember you a star, like in the image bellow.\n",
    "\n",
    "![](./images/star.jpg)\n",
    "\n",
    "This kind of schema was choiced by me because of its simplicity during the modeling and due its format the querys are smaller than in another models making it easy to access the data contained in the tables.\n",
    "\n",
    "\n",
    "### Fact table\n",
    "\n",
    "The fact table is responsable for storage the events occured in the bussiness process, in this case our fact table will store all the transactions ocurred in the accounts, including pix transactions. The field type was included in the new table, this field is responsable to store the transaction method that the user used execute the transaction, in this case it could be pix and standard transfers, but like in the file provided for nubank it can suporte another kind of methods, like whatsapp, phone recharge and others. The field in or out was included to make the distinction between the transactions that were made to the account(money arriving) or from the account(money leaving). The customer_id was included to make the connection with the costumer dimension table. In the image bellow we can see the result of our fact table.\n",
    "\n",
    "![](./images/fact_table.jpg)\n",
    "\n",
    "\n",
    "### Dimensional tables\n",
    "\n",
    "The dimensional tables are responsable for the descriptive concept, it contains the textual data needed to describe the event occured in the process. Each dimensional table is connected with the fact table using a primary key. In our case are necessary three dimensional tables that describe the transaction process, the table with customers informations, the table with accounts information and the table with time information. I believe that with this three table we can describe the transaction process very clearly and easier than before.\n",
    "\n",
    "**Customer dimensional table:** In this table all the data related the customer is stored.\n",
    "\n",
    "![](./images/dim_customer.jpg)\n",
    "\n",
    "**Accounts dimensional table:** This table have all the data related the accounts. It was not changed.\n",
    "\n",
    "![](./images/dim_accounts.jpg)\n",
    "\n",
    "**Time dimensional table:** In this table is the data responsable for describe the time, like year, month, day etc.\n",
    "\n",
    "![](./images/dim_time.jpg)\n",
    "\n",
    "### Final schema\n",
    "\n",
    "Puting all the described tables above together with the respectives conections between fact and dimension tables we can see how simple the schema has become. A connection between accounts table and customers table was necessary in order to return the data related from customer and his account.\n",
    "\n",
    "\n",
    "![](./images/final_schema.jpg)\n",
    "\n",
    "\n",
    "Note: Part of the definitions used to make this data modeling were based on the book [The Data Warehouse Toolkit: The Definitive Guide to Dimensional Modeling](https://www.oreilly.com/library/view/the-data-warehouse/9781118530801/)\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Comparing performance\n",
    "\n",
    "Using the Jane`s case where she wants to get the balance from the accounts, with the old data warehouse schema the query used is performed in approximately 1 second, with the new schema the new query was performed in approximately 0,66 second. With this we reduced the time to execute the query in 34%, in a big volume os transactions this reduction becomes very significant."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "start = time.time()\n",
    "balance_new = pd.read_sql('''SELECT mes, account_id, SUM(ENTRADA) AS entradas_total, SUM(SAIDA) AS saidas_total, (SUM(ENTRADA)-SUM(SAIDA)) as balanco\n",
    "                                FROM (SELECT account_id, \n",
    "                                        CASE in_or_out WHEN 'out' then amount ELSE 0 END as SAIDA, \n",
    "                                        CASE in_or_out WHEN  'in' then amount ELSE 0 END as ENTRADA,\n",
    "                                        DATE_PART('month', to_timestamp(CAST(completed_at as numeric)/1000)) MES\n",
    "                                        FROM fact_movements_table\n",
    "                                        WHERE (status = 'completed') AND (DATE_PART('year', to_timestamp(CAST(completed_at as numeric)/1000)) = 2020)) AS transactions\n",
    "                            GROUP BY mes, account_id\n",
    "                            ORDER BY mes, account_id\n",
    "\n",
    "\n",
    "''', engine)\n",
    "end = time.time()\n",
    "print(\"Time to query execution: \",end - start)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 19,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Time to query execution:  0.6868879795074463\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "    mes         account_id  entradas_total  saidas_total  balanco\n",
       "0   1.0   2569200459575096         6622.15       1826.30  4795.85\n",
       "1   1.0   2572645138169593         3314.38       1467.31  1847.07\n",
       "2   1.0   2969674447809961         9013.62       3569.48  5444.14\n",
       "3   1.0   5756422114496119         2926.91       2170.55   756.36\n",
       "4   1.0   5763135580788529         6460.05      11030.11 -4570.06\n",
       "5   1.0   6731171884115662         9660.82       5316.85  4343.97\n",
       "6   1.0   6754575908057409         5780.74          0.00  5780.74\n",
       "7   1.0   6759884497455352         4308.37       1542.19  2766.18\n",
       "8   1.0   7106839639082916         3062.88       1402.65  1660.23\n",
       "9   1.0   7399497589386025         1319.22          0.00  1319.22\n",
       "10  1.0   8390860978077109         7559.18      17064.75 -9505.57\n",
       "11  1.0   9682140161583058         6125.78      11087.89 -4962.11\n",
       "12  1.0  11972044772734216         4480.96        759.50  3721.46\n",
       "13  1.0  14823956517654278        11269.08       4940.66  6328.42\n",
       "14  1.0  14960923507500004         2365.23       4424.75 -2059.52\n",
       "15  1.0  16488801170250194         1382.95          0.00  1382.95\n",
       "16  1.0  17010643349938116         1446.35       1812.93  -366.58\n",
       "17  1.0  17352812883658752         8757.83      13354.52 -4596.69\n",
       "18  1.0  18661351842096488         1148.46       5642.76 -4494.30\n",
       "19  1.0  21312345113145264        10272.73       2697.03  7575.70"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mes</th>\n      <th>account_id</th>\n      <th>entradas_total</th>\n      <th>saidas_total</th>\n      <th>balanco</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.0</td>\n      <td>2569200459575096</td>\n      <td>6622.15</td>\n      <td>1826.30</td>\n      <td>4795.85</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.0</td>\n      <td>2572645138169593</td>\n      <td>3314.38</td>\n      <td>1467.31</td>\n      <td>1847.07</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.0</td>\n      <td>2969674447809961</td>\n      <td>9013.62</td>\n      <td>3569.48</td>\n      <td>5444.14</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.0</td>\n      <td>5756422114496119</td>\n      <td>2926.91</td>\n      <td>2170.55</td>\n      <td>756.36</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.0</td>\n      <td>5763135580788529</td>\n      <td>6460.05</td>\n      <td>11030.11</td>\n      <td>-4570.06</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1.0</td>\n      <td>6731171884115662</td>\n      <td>9660.82</td>\n      <td>5316.85</td>\n      <td>4343.97</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>1.0</td>\n      <td>6754575908057409</td>\n      <td>5780.74</td>\n      <td>0.00</td>\n      <td>5780.74</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>1.0</td>\n      <td>6759884497455352</td>\n      <td>4308.37</td>\n      <td>1542.19</td>\n      <td>2766.18</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>1.0</td>\n      <td>7106839639082916</td>\n      <td>3062.88</td>\n      <td>1402.65</td>\n      <td>1660.23</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>1.0</td>\n      <td>7399497589386025</td>\n      <td>1319.22</td>\n      <td>0.00</td>\n      <td>1319.22</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>1.0</td>\n      <td>8390860978077109</td>\n      <td>7559.18</td>\n      <td>17064.75</td>\n      <td>-9505.57</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>1.0</td>\n      <td>9682140161583058</td>\n      <td>6125.78</td>\n      <td>11087.89</td>\n      <td>-4962.11</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>1.0</td>\n      <td>11972044772734216</td>\n      <td>4480.96</td>\n      <td>759.50</td>\n      <td>3721.46</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>1.0</td>\n      <td>14823956517654278</td>\n      <td>11269.08</td>\n      <td>4940.66</td>\n      <td>6328.42</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>1.0</td>\n      <td>14960923507500004</td>\n      <td>2365.23</td>\n      <td>4424.75</td>\n      <td>-2059.52</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>1.0</td>\n      <td>16488801170250194</td>\n      <td>1382.95</td>\n      <td>0.00</td>\n      <td>1382.95</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>1.0</td>\n      <td>17010643349938116</td>\n      <td>1446.35</td>\n      <td>1812.93</td>\n      <td>-366.58</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>1.0</td>\n      <td>17352812883658752</td>\n      <td>8757.83</td>\n      <td>13354.52</td>\n      <td>-4596.69</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>1.0</td>\n      <td>18661351842096488</td>\n      <td>1148.46</td>\n      <td>5642.76</td>\n      <td>-4494.30</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>1.0</td>\n      <td>21312345113145264</td>\n      <td>10272.73</td>\n      <td>2697.03</td>\n      <td>7575.70</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "balance_new.head(20)"
   ]
  },
  {
   "source": [
    "## 3. Come up with a strategy to implement the warehouse changes you proposed\n",
    "\n",
    "In order to change our data warehouse above with your suggestions, we need to come up with a\n",
    "migration plan since many people are consuming data from it. Which strategy would you propose\n",
    "in order to implement those changes?\n",
    "\n",
    "\n",
    "### Resolution:\n",
    "\n",
    "When talking about data migration, I believe that there are two main approachs to do this, the **Big Bang data migration** and **Trickle data migration**. There are a few differences between this two methods, so is very important to define the right choise to our migration project.\n",
    "\n",
    "**Big bang data migration:** Is a method where you move all the data in one operation with a limited window of time. The data stays down while the ETL process is happening.\n",
    "\n",
    "Advantages: Spend less time, less costly and less complex.\n",
    "\n",
    "Disavantages: Requires downtime of your data, risk of expensive failure.\n",
    "\n",
    "**Trickle data migration:** \n",
    "\n",
    "Advantages: Less susceptible to big failures, no data down time.\n",
    "\n",
    "Disavantages: More expensive, need more time to be implemented.\n",
    "\n",
    "\n",
    "Considering that the data transactions is very important to our bussiness analysts and we can`t have down time on this, a most aceptable choice is use the **Trickle data migration** approach.\n",
    "\n",
    "Using a kind of agile projects management method to make this data migration we can define five main steps:\n",
    "\n",
    "1. Initiation\n",
    "\n",
    "2. Planning\n",
    "\n",
    "3. Execution\n",
    "\n",
    "4. Performance/Monitoring\n",
    "\n",
    "5. Project close\n",
    "\n",
    "\n",
    "**1. Initiation**\n",
    "\n",
    "In this first step we need to mark the beginning of the project, make the macro desicions, define the stakeholders and show then the project, impacts and why this project is happening. In our case we need to warn the bussiness analysts and the other interested areas about the changes and the impacts.\n",
    "\n",
    "**2. Planning**\n",
    "\n",
    "Here we need to define the scope and document it, define the derivables, project`s timeline and the budget. Thinking about data migration is necessary to define the data standards to avoid problems in the next steps and\n",
    "\n",
    "**3. Execution**\n",
    "\n",
    "This stage is where the outputs are made, here we have to deliver what was defined in the past stages of the project. In a data migration the possible outputs to this fase are the data audition, the data quality verification, data backup to keep our data safe from possible errors, the migration of the data using an ETL process.The tests are present during the execution stage, in the Trickle data migration is necessary make tests in each portion of migrated data to avoid or fiz possible problems.\n",
    "\n",
    "**4. Performance/Monitoring**\n",
    "\n",
    "During this stage is necessary to do meassurements about the performance of the new data warehouse, is important to have KPIs(key performance indicators) to compare how good the things are going. A data audition is very important in this stage, this could be an confirmation by the bussiness analysts about the right migration of the data.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 4. Propose metrics to track PIX performance and its impact on Nubank. Feel free to come up with any metrics you consider relevant\n",
    "\n",
    "Jane's friend, Pepino, wants to know how well PIX is doing inside Nubank. For that, he wants your\n",
    "help to come up with indicators that can be used to track the performance of the product. Which\n",
    "metrics would you suggest to track it and why?\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Resolution\n",
    "\n",
    "**What is PIX ? What is its purpose ?**\n",
    "\n",
    "Accordingly with [Central Bank of Brazil](https://www.bcb.gov.br/estabilidadefinanceira/pix), PIX is a instant payment method, where the transaction is completed in a few seconds, any time of the day."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Trying to help Pepino and looking in the pix definition we can define our first metric. PIX sohuld be a instant transaction, so let`s see how many time the completed transactions are spending between the request and its completion."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Time to query execution:  0.36342310905456543\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "time_new = pd.read_sql('''SELECT *\n",
    "                                FROM fact_movements_table AS f\n",
    "                                JOIN d_time_table AS t ON t.time_id = CAST(f.requested_at AS bigint) AND t.time_id = CAST(f.completed_at AS NUMERIC)\n",
    "                                WHERE f.status = 'completed'\n",
    "                                \n",
    "\n",
    "\n",
    "''', engine)\n",
    "end = time.time()\n",
    "print(\"Time to query execution: \",end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                    id           account_id          customer_id in_or_out  \\\n",
       "0  2337816270461275648   285954202652059168  2764997088848309760        in   \n",
       "1  1572011189238133760  2582312454490957824   960422580921375360        in   \n",
       "2   140652736007160848  1193234806014966272  1334445880917911808        in   \n",
       "3   945569752989672960  1540994009165377536  2289202895067939840        in   \n",
       "4  1275610492486608896  1027723092389070208  2844618031444976128       out   \n",
       "\n",
       "  type   amount   requested_at     completed_at     status        time_id  \\\n",
       "0  pix   130.37  1594892787980  1594892787980.0  completed  1594892787980   \n",
       "1  pix   772.61  1578158343570  1578158343570.0  completed  1578158343570   \n",
       "2  pix  1956.72  1596428182020  1596428182020.0  completed  1596428182020   \n",
       "3  pix  1875.35  1607314786290  1607314786290.0  completed  1607314786290   \n",
       "4  pix  1805.53  1607244264040  1607244264040.0  completed  1607244264040   \n",
       "\n",
       "         action_timestamp    week_id  month_id  year_id  weekday_id  \n",
       "0 2020-07-16 09:46:27.980  102428196      3549  1024140        1521  \n",
       "1 2020-01-04 17:19:03.570  102414000       507  1024140        2535  \n",
       "2 2020-08-03 04:16:22.020  102429717      4056  1024140           0  \n",
       "3 2020-12-07 04:19:46.290  102438843      6084  1024140           0  \n",
       "4 2020-12-06 08:44:24.040  102438336      6084  1024140        3042  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>account_id</th>\n      <th>customer_id</th>\n      <th>in_or_out</th>\n      <th>type</th>\n      <th>amount</th>\n      <th>requested_at</th>\n      <th>completed_at</th>\n      <th>status</th>\n      <th>time_id</th>\n      <th>action_timestamp</th>\n      <th>week_id</th>\n      <th>month_id</th>\n      <th>year_id</th>\n      <th>weekday_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2337816270461275648</td>\n      <td>285954202652059168</td>\n      <td>2764997088848309760</td>\n      <td>in</td>\n      <td>pix</td>\n      <td>130.37</td>\n      <td>1594892787980</td>\n      <td>1594892787980.0</td>\n      <td>completed</td>\n      <td>1594892787980</td>\n      <td>2020-07-16 09:46:27.980</td>\n      <td>102428196</td>\n      <td>3549</td>\n      <td>1024140</td>\n      <td>1521</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1572011189238133760</td>\n      <td>2582312454490957824</td>\n      <td>960422580921375360</td>\n      <td>in</td>\n      <td>pix</td>\n      <td>772.61</td>\n      <td>1578158343570</td>\n      <td>1578158343570.0</td>\n      <td>completed</td>\n      <td>1578158343570</td>\n      <td>2020-01-04 17:19:03.570</td>\n      <td>102414000</td>\n      <td>507</td>\n      <td>1024140</td>\n      <td>2535</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>140652736007160848</td>\n      <td>1193234806014966272</td>\n      <td>1334445880917911808</td>\n      <td>in</td>\n      <td>pix</td>\n      <td>1956.72</td>\n      <td>1596428182020</td>\n      <td>1596428182020.0</td>\n      <td>completed</td>\n      <td>1596428182020</td>\n      <td>2020-08-03 04:16:22.020</td>\n      <td>102429717</td>\n      <td>4056</td>\n      <td>1024140</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>945569752989672960</td>\n      <td>1540994009165377536</td>\n      <td>2289202895067939840</td>\n      <td>in</td>\n      <td>pix</td>\n      <td>1875.35</td>\n      <td>1607314786290</td>\n      <td>1607314786290.0</td>\n      <td>completed</td>\n      <td>1607314786290</td>\n      <td>2020-12-07 04:19:46.290</td>\n      <td>102438843</td>\n      <td>6084</td>\n      <td>1024140</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1275610492486608896</td>\n      <td>1027723092389070208</td>\n      <td>2844618031444976128</td>\n      <td>out</td>\n      <td>pix</td>\n      <td>1805.53</td>\n      <td>1607244264040</td>\n      <td>1607244264040.0</td>\n      <td>completed</td>\n      <td>1607244264040</td>\n      <td>2020-12-06 08:44:24.040</td>\n      <td>102438336</td>\n      <td>6084</td>\n      <td>1024140</td>\n      <td>3042</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "time_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}